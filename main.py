# ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ libraries ‡∂Ü‡∂∫‡∑è‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
import os
import streamlit as st
from PyPDF2 import PdfReader
from groq import Groq
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Groq API client ‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
client = Groq(api_key="your-api-key")

# Streamlit UI ‡∑É‡∑ê‡∂ö‡∑É‡∑ì‡∂∏
st.title("üìÑ PDF ‡∂¥‡∑ä‚Äç‡∂ª‡∑Å‡∑ä‡∂±-‡∂¥‡∑í‡∑Ö‡∑í‡∂≠‡∑î‡∂ª‡∑î ‡∂¥‡∂Ø‡∑ä‡∂∞‡∂≠‡∑í‡∂∫ (RAG)")
st.write("PDF ‡∂ú‡∑ú‡∂±‡∑î‡∑Ä‡∂ö‡∑ä ‡∂ã‡∂©‡∑î‡∂ú‡∂≠ ‡∂ö‡∂ª ‡∂î‡∂∂‡∑ö ‡∂¥‡∑ä‚Äç‡∂ª‡∑Å‡∑ä‡∂± ‡∂Ö‡∑É‡∂±‡∑ä‡∂±")

# PDF ‡∂ú‡∑ú‡∂±‡∑î‡∑Ä ‡∂ã‡∂©‡∑î‡∂ú‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
uploaded_file = st.file_uploader("PDF ‡∂ú‡∑ú‡∂±‡∑î‡∑Ä ‡∂≠‡∑ù‡∂ª‡∂±‡∑ä‡∂±", type="pdf")

if uploaded_file is not None:
    # PDF ‡∂ú‡∑ú‡∂±‡∑î‡∑Ä‡∑ô‡∂±‡∑ä ‡∂¥‡∑è‡∂®‡∂∫ ‡∂ã‡∂ö‡∑Ñ‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏
    pdf_reader = PdfReader(uploaded_file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() or ""  # None ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∑Ä‡∑Ö‡∂ö‡∑í‡∂±‡∑ä‡∂±

    # ‡∂¥‡∑è‡∂®‡∂∫ ‡∂ö‡∑î‡∂©‡∑è ‡∂ö‡∑ú‡∂ß‡∑É‡∑ä‡∑Ä‡∂Ω‡∂ß (chunks) ‡∂∂‡∑ô‡∂Ø‡∑ì‡∂∏
    # CharacterTextSplitter ‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∂ß ‡∑É‡∂ª‡∂Ω ‡∂∂‡∑ô‡∂Ø‡∑ì‡∂∏‡∂ö‡∑ä
    chunk_size = 1000
    chunk_overlap = 200
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - chunk_overlap

    # Embeddings ‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
    embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = embedding_model.encode(chunks, show_progress_bar=True)

    # FAISS vector store ‡∑É‡∑ê‡∂ö‡∑É‡∑ì‡∂∏
    dimension = embeddings.shape[1]  # Embedding dimension
    index = faiss.IndexFlatL2(dimension)  # L2 ‡∂Ø‡∑î‡∂ª‡∂∏‡∑í‡∂≠‡∑í‡∂ö ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∂∫
    index.add(embeddings)  # Embeddings vector store ‡∂ë‡∂ö‡∂ß ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏

    # ‡∂¥‡∑ä‚Äç‡∂ª‡∑Å‡∑ä‡∂±‡∂∫ ‡∂á‡∂≠‡∑î‡∂Ω‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
    user_question = st.text_input("‡∂î‡∂∂‡∑ö ‡∂¥‡∑ä‚Äç‡∂ª‡∑Å‡∑ä‡∂±‡∂∫ ‡∂á‡∂≠‡∑î‡∂Ω‡∂≠‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±:")

    if user_question:
        # ‡∂¥‡∑ä‚Äç‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∑ö embedding ‡∂ú‡∂´‡∂±‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
        question_embedding = embedding_model.encode([user_question])[0]

        # FAISS ‡∑Ñ‡∑í similarity search ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
        D, I = index.search(np.array([question_embedding]), k=4)  # Top 4 similar chunks
        context = "\n".join([chunks[i] for i in I[0]])

        # Groq API ‡∑Ñ‡∂ª‡∑Ñ‡∑è ‡∂¥‡∑í‡∑Ö‡∑í‡∂≠‡∑î‡∂ª ‡∂¢‡∂±‡∂±‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {
                    "role": "system",
                    "content": f"answer questions using the given source :\n\n{context}"
                },
                {
                    "role": "user",
                    "content": user_question
                }
            ],
            temperature=0.7,
            max_tokens=1024,
            top_p=1,
            stream=True,
            stop=None,
        )

        # Streamlit ‡∑Ñ‡∑í ‡∂¥‡∑í‡∑Ö‡∑í‡∂≠‡∑î‡∂ª ‡∂¥‡∑ô‡∂±‡∑ä‡∑Ä‡∑ì‡∂∏
        st.write("‡∂¥‡∑í‡∑Ö‡∑í‡∂≠‡∑î‡∂ª:")
        answer_container = st.empty()
        full_answer = ""
        for chunk in completion:
            chunk_content = chunk.choices[0].delta.content or ""
            full_answer += chunk_content
            answer_container.write(full_answer)
